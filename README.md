# Seq2Seq-Attention-Mechanism-Demo
A comprehensive NLP project demonstrating the impact of the **Attention Mechanism** on Sequence-to-Sequence (Seq2Seq) models. This project compares a baseline GRU Encoder-Decoder against an Attention-based model (Bahdanau Attention) on a character-level string reversal task.
