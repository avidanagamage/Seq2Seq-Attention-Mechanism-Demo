# Seq2Seq Attention Mechanism Demo

A comprehensive NLP project demonstrating the impact of the **Attention Mechanism** on Sequence-to-Sequence (Seq2Seq) models. This project compares a baseline GRU Encoder-Decoder against an Attention-based model (Bahdanau Attention) on a character-level string reversal task.

It includes a **Streamlit** web application to visualize predictions and attention heatmaps interactively.

## ðŸ“‚ Project Structure

```text
NLP Project/
â”œâ”€â”€ data/                  # Dataset generation and utilities
â”‚   â”œâ”€â”€ generate_dataset.py
â”‚   â”œâ”€â”€ dataset_utils.py
â”‚   â””â”€â”€ char_vocab.json
â”œâ”€â”€ models/                # Model architectures and training scripts
â”‚   â”œâ”€â”€ baseline/          # Standard Seq2Seq (Encoder-Decoder)
â”‚   â””â”€â”€ attention/         # Seq2Seq with Bahdanau Attention
â”œâ”€â”€ demo_app/              # Streamlit Web Application
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ sample_inputs.json
â””â”€â”€ visualization/         # Scripts to generate training heatmaps
    â””â”€â”€ save_attention_heatmaps.py
